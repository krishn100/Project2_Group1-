{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # To show all rows\n",
    "pd.set_option('display.max_columns', None)  # To show all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOOGLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSV files that contain SMA, EMA, and BB/RSI inidcators based on ratio data\n",
    "googl_df = pd.read_csv(\"googl_signals.csv\", infer_datetime_format=True, index_col=\"Date\", parse_dates=True)\n",
    "nvda_df = pd.read_csv(\"nvda_signals.csv\", infer_datetime_format=True, index_col=\"Date\", parse_dates=True)\n",
    "mmm_df = pd.read_csv(\"mmm_signals.csv\", infer_datetime_format=True, index_col=\"Date\", parse_dates=True)\n",
    "pg_df = pd.read_csv(\"pg_signals.csv\", infer_datetime_format=True, index_col=\"Date\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for machine learning, scaling, resampling and classification reports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import pdfkit\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GOOGL Adj. Close', 'GOOGL P/S (LTM)', 'GOOGL P/FCF (LTM)',\n",
       "       'GOOGL P/E (LTM)', 'GOOGL Debt/Equity (LTM)', 'actual_returns',\n",
       "       'ps_sma_fast', 'ps_sma_fast30', 'ps_sma_slow', 'ps_ema_fast',\n",
       "       ...\n",
       "       'ps_Portfolio Cumulative Returns_sma30',\n",
       "       'ps_Portfolio Cumulative Returns_ema30',\n",
       "       'pfcf_Portfolio Cumulative Returns_sma',\n",
       "       'pfcf_Portfolio Cumulative Returns_ema',\n",
       "       'pfcf_Portfolio Cumulative Returns_sma30',\n",
       "       'pfcf_Portfolio Cumulative Returns_ema30',\n",
       "       'pe_Portfolio Cumulative Returns_sma',\n",
       "       'pe_Portfolio Cumulative Returns_ema',\n",
       "       'pe_Portfolio Cumulative Returns_sma30',\n",
       "       'pe_Portfolio Cumulative Returns_ema30'],\n",
       "      dtype='object', length=132)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "googl_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X)\n",
    "features = ['GOOGL P/S (LTM)', 'GOOGL P/FCF (LTM)', 'GOOGL P/E (LTM)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y variables\n",
    "y_variables = ['ps_Entry/Exit_sma30', 'ps_Entry/Exit_ema30', 'pfcf_Entry/Exit_sma30',\n",
    "               'pfcf_Entry/Exit_ema30', 'pe_Entry/Exit_sma30', 'pe_Entry/Exit_ema30']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classification reports for 'ps_Entry/Exit_sma30' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'ps_Entry/Exit_ema30' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pfcf_Entry/Exit_sma30' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pfcf_Entry/Exit_ema30' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pe_Entry/Exit_sma30' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pe_Entry/Exit_ema30' ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "logistic_regression = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "models = {'Naive Bayes': GaussianNB(),\n",
    "          'Random Forest Classifier': RandomForestClassifier(random_state=42),\n",
    "          'Logistic Regression': logistic_regression}\n",
    "\n",
    "# Create empty DataFrames to store the classification reports\n",
    "reports_df_test = pd.DataFrame()\n",
    "reports_df_train = pd.DataFrame()\n",
    "\n",
    "# Loop through each y variable\n",
    "for y_var in y_variables:\n",
    "    print(f\"\\n--- Classification reports for '{y_var}' ---\\n\")\n",
    "    \n",
    "    # Drop NaN values from X and y for the current y variable\n",
    "    data_cleaned = googl_df.dropna(subset=features + [y_var])\n",
    "    X = data_cleaned[features]\n",
    "    y = data_cleaned[y_var]\n",
    "\n",
    "    # Apply Random Oversampling to address class imbalance\n",
    "    oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Loop through each model\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "        # Compute and store classification report for testing data\n",
    "        report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "        report_df_test = pd.DataFrame(report_test).transpose()\n",
    "        report_df_test['y_variable'] = y_var\n",
    "        report_df_test['model'] = model_name\n",
    "        reports_df_test = pd.concat([reports_df_test, report_df_test])\n",
    "\n",
    "        # Make predictions on the training data\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "        # Compute and store classification report for training data\n",
    "        report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "        report_df_train = pd.DataFrame(report_train).transpose()\n",
    "        report_df_train['y_variable'] = y_var\n",
    "        report_df_train['model'] = model_name\n",
    "        reports_df_train = pd.concat([reports_df_train, report_df_train])\n",
    "\n",
    "# Export the DataFrames containing classification reports to separate CSV files\n",
    "reports_df_test.to_csv('testing_classification_reports_GOOGLE_30.csv', index=True)\n",
    "reports_df_train.to_csv('training_classification_reports_GOOGLE_30.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NVDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) \n",
    "features = ['NVDA P/S (LTM)', 'NVDA P/FCF (LTM)', 'NVDA P/E (LTM)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y variables\n",
    "y_variables = ['ps_Entry/Exit_sma', 'ps_Entry/Exit_ema', 'pfcf_Entry/Exit_sma',\n",
    "               'pfcf_Entry/Exit_ema', 'pe_Entry/Exit_sma', 'pe_Entry/Exit_ema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classification reports for 'ps_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'ps_Entry/Exit_ema' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pfcf_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pfcf_Entry/Exit_ema' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pe_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pe_Entry/Exit_ema' ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "logistic_regression = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "models = {'Naive Bayes': GaussianNB(),\n",
    "          'Random Forest Classifier': RandomForestClassifier(random_state=42),\n",
    "          'Logistic Regression': logistic_regression}\n",
    "\n",
    "# Create empty DataFrames to store the classification reports\n",
    "reports_df_test = pd.DataFrame()\n",
    "reports_df_train = pd.DataFrame()\n",
    "\n",
    "# Loop through each y variable\n",
    "for y_var in y_variables:\n",
    "    print(f\"\\n--- Classification reports for '{y_var}' ---\\n\")\n",
    "    \n",
    "    # Drop NaN values from X and y for the current y variable\n",
    "    data_cleaned = nvda_df.dropna(subset=features + [y_var])\n",
    "    X = data_cleaned[features]\n",
    "    y = data_cleaned[y_var]\n",
    "\n",
    "    # Apply Random Oversampling to address class imbalance\n",
    "    oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Loop through each model\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "        # Compute and store classification report for testing data\n",
    "        report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "        report_df_test = pd.DataFrame(report_test).transpose()\n",
    "        report_df_test['y_variable'] = y_var\n",
    "        report_df_test['model'] = model_name\n",
    "        reports_df_test = pd.concat([reports_df_test, report_df_test])\n",
    "\n",
    "        # Make predictions on the training data\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "        # Compute and store classification report for training data\n",
    "        report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "        report_df_train = pd.DataFrame(report_train).transpose()\n",
    "        report_df_train['y_variable'] = y_var\n",
    "        report_df_train['model'] = model_name\n",
    "        reports_df_train = pd.concat([reports_df_train, report_df_train])\n",
    "\n",
    "# Export the DataFrames containing classification reports to separate CSV files\n",
    "reports_df_test.to_csv('testing_classification_reports_NVDA_30.csv', index=True)\n",
    "reports_df_train.to_csv('training_classification_reports_NVDA_30.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MMM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) \n",
    "features = ['MMM P/S (LTM)', 'MMM P/FCF (LTM)', 'MMM P/E (LTM)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y variables\n",
    "y_variables = ['ps_Entry/Exit_sma', 'ps_Entry/Exit_ema', 'pfcf_Entry/Exit_sma',\n",
    "               'pfcf_Entry/Exit_ema', 'pe_Entry/Exit_sma', 'pe_Entry/Exit_ema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classification reports for 'ps_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'ps_Entry/Exit_ema' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pfcf_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pfcf_Entry/Exit_ema' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pe_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pe_Entry/Exit_ema' ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "logistic_regression = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "models = {'Naive Bayes': GaussianNB(),\n",
    "          'Random Forest Classifier': RandomForestClassifier(random_state=42),\n",
    "          'Logistic Regression': logistic_regression}\n",
    "\n",
    "# Create empty DataFrames to store the classification reports\n",
    "reports_df_test = pd.DataFrame()\n",
    "reports_df_train = pd.DataFrame()\n",
    "\n",
    "# Loop through each y variable\n",
    "for y_var in y_variables:\n",
    "    print(f\"\\n--- Classification reports for '{y_var}' ---\\n\")\n",
    "    \n",
    "    # Drop NaN values from X and y for the current y variable\n",
    "    data_cleaned = mmm_df.dropna(subset=features + [y_var])\n",
    "    X = data_cleaned[features]\n",
    "    y = data_cleaned[y_var]\n",
    "\n",
    "    # Apply Random Oversampling to address class imbalance\n",
    "    oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Loop through each model\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "        # Compute and store classification report for testing data\n",
    "        report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "        report_df_test = pd.DataFrame(report_test).transpose()\n",
    "        report_df_test['y_variable'] = y_var\n",
    "        report_df_test['model'] = model_name\n",
    "        reports_df_test = pd.concat([reports_df_test, report_df_test])\n",
    "\n",
    "        # Make predictions on the training data\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "        # Compute and store classification report for training data\n",
    "        report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "        report_df_train = pd.DataFrame(report_train).transpose()\n",
    "        report_df_train['y_variable'] = y_var\n",
    "        report_df_train['model'] = model_name\n",
    "        reports_df_train = pd.concat([reports_df_train, report_df_train])\n",
    "\n",
    "# Export the DataFrames containing classification reports to separate CSV files\n",
    "reports_df_test.to_csv('testing_classification_reports_MMM_30.csv', index=True)\n",
    "reports_df_train.to_csv('training_classification_reports_MMM_30.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) \n",
    "features = ['PG P/S (LTM)', 'PG P/FCF (LTM)', 'PG P/E (LTM)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y variables\n",
    "y_variables = ['ps_Entry/Exit_sma', 'ps_Entry/Exit_ema', 'pfcf_Entry/Exit_sma',\n",
    "               'pfcf_Entry/Exit_ema', 'pe_Entry/Exit_sma', 'pe_Entry/Exit_ema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classification reports for 'ps_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'ps_Entry/Exit_ema' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pfcf_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pfcf_Entry/Exit_ema' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pe_Entry/Exit_sma' ---\n",
      "\n",
      "\n",
      "--- Classification reports for 'pe_Entry/Exit_ema' ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "logistic_regression = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "models = {'Naive Bayes': GaussianNB(),\n",
    "          'Random Forest Classifier': RandomForestClassifier(random_state=42),\n",
    "          'Logistic Regression': logistic_regression}\n",
    "\n",
    "# Create empty DataFrames to store the classification reports\n",
    "reports_df_test = pd.DataFrame()\n",
    "reports_df_train = pd.DataFrame()\n",
    "\n",
    "# Loop through each y variable\n",
    "for y_var in y_variables:\n",
    "    print(f\"\\n--- Classification reports for '{y_var}' ---\\n\")\n",
    "    \n",
    "    # Drop NaN values from X and y for the current y variable\n",
    "    data_cleaned = pg_df.dropna(subset=features + [y_var])\n",
    "    X = data_cleaned[features]\n",
    "    y = data_cleaned[y_var]\n",
    "\n",
    "    # Apply Random Oversampling to address class imbalance\n",
    "    oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Loop through each model\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "        # Compute and store classification report for testing data\n",
    "        report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "        report_df_test = pd.DataFrame(report_test).transpose()\n",
    "        report_df_test['y_variable'] = y_var\n",
    "        report_df_test['model'] = model_name\n",
    "        reports_df_test = pd.concat([reports_df_test, report_df_test])\n",
    "\n",
    "        # Make predictions on the training data\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "        # Compute and store classification report for training data\n",
    "        report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "        report_df_train = pd.DataFrame(report_train).transpose()\n",
    "        report_df_train['y_variable'] = y_var\n",
    "        report_df_train['model'] = model_name\n",
    "        reports_df_train = pd.concat([reports_df_train, report_df_train])\n",
    "\n",
    "# Export the DataFrames containing classification reports to separate CSV files\n",
    "reports_df_test.to_csv('testing_classification_reports_PG_30.csv', index=True)\n",
    "reports_df_train.to_csv('training_classification_reports_PG_30.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotradinml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
